{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#w1_initial=np.random.normal(size=(28*28*1,10)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    def next_batch(batch_size , image , label):\n",
    "\n",
    "        a=np.random.randint(np.shape(image)[0] -batch_size)\n",
    "        batch_x = image[a:a+batch_size,:]\n",
    "        batch_y= label[a:a+batch_size,:]\n",
    "        return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_img=np.load('/home/user01/notebook/Mnist_Data/train_img.npy')\n",
    "train_lab=np.load('/home/user01/notebook/Mnist_Data/train_lab.npy')\n",
    "test_img=np.load('/home/user01/notebook/Mnist_Data/test_img.npy')\n",
    "test_lab=np.load('/home/user01/notebook/Mnist_Data/test_lab.npy')\n",
    "val_img=np.load('/home/user01/notebook/Mnist_Data/val_img.npy')\n",
    "val_lab=np.load('/home/user01/notebook/Mnist_Data/val_lab.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.shape(train_img)\n",
    "np.shape(val_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img=np.reshape(val_img[0], newshape=[28,28])\n",
    "lab=val_lab[0]\n",
    "print lab\n",
    "\n",
    "plt.imshow(img, cmap='binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_=tf.placeholder(tf.float32,shape=[None,28*28*1])\n",
    "y_=tf.placeholder(tf.float32,shape=[None,10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w1=tf.Variable(w1_initial)\n",
    "mean_w1 = tf.reduce_mean(w1)\n",
    "b1=tf.Variable(w1_initial)\n",
    "z1=tf.matmul(x_,w1)\n",
    "l1=tf.nn.relu(z1)\n",
    "y=tf.nn.softmax(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_entropy =tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "\n",
    "\n",
    "train_step =tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.arg_max(y,1) , tf.arg_max(y_ , 1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction , tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zs , BNs , acc, acc_BNs = [],[],[],[]\n",
    "\n",
    "\n",
    "init=tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc,zs,acc_val , zs_val,w1_mean_list =[],[],[],[],[]\n",
    "start_time=time.time()\n",
    "for i in range(200000):\n",
    "\n",
    "    batch_xs , batch_ys =next_batch( 60 , train_img , train_lab) \n",
    "    sess.run( train_step , feed_dict ={ x_: batch_xs , y_ :batch_ys })\n",
    "    if i%50 is 0:\n",
    "        train_res = sess.run([accuracy , z1 ] , feed_dict = {x_:batch_xs, y_ : batch_ys})\n",
    "        val_res   = sess.run([accuracy , z1 ] , feed_dict = {x_:val_img, y_ : val_lab})\n",
    "        mean   = sess.run(mean_w1 , feed_dict = {x_:val_img, y_ : val_lab})\n",
    "        \n",
    "        print 'step:',i, 'training',train_res[0],'           validation' , val_res[0] \n",
    "        acc.append(train_res[0])\n",
    "        zs.append(np.mean(train_res[1] , axis=0))\n",
    "        w1_mean_list.append(mean)\n",
    "        print \n",
    "        acc_val.append(val_res[0])\n",
    "        zs_val.append(np.mean(val_res[1] , axis=0))            \n",
    "end_time=time.time()\n",
    "zs , val_zs , acc, acc_val = np.array(zs) , np.array(zs_val) , np.array(acc) , np.array(acc_val)\n",
    "#when we test , through using above 4 parameter we get more higher accuracy \n",
    "test_res = sess.run([accuracy , z1 ] , feed_dict = {x_:test_img, y_ : test_lab})\n",
    "print 'test accuracy : ' , test_res[0] \n",
    "print 'final output ' , test_res[1]\n",
    "print 'The time taken by Training: ',end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_=tf.placeholder(tf.float32 , shape=[None,28*28*1])\n",
    "y_=tf.placeholder(tf.float32 , shape=[None , 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Layer1 without Batch Normalization  \n",
    "w1_initial=np.random.normal(size=(28*28*1,10)).astype(np.float32) #웨이치를 정한다.\n",
    "w1 = tf.Variable(w1_initial)\n",
    "mean_w1 = tf.reduce_mean(w1)\n",
    "\n",
    "#바이어스를 만든다. 모든 웨이치에 매핑되는 바이어스이다.\n",
    "b1=tf.Variable(tf.zeros([10]))\n",
    "z1=tf.matmul(x_,w1)+b1\n",
    "l1=tf.nn.relu(z1) #음수값을 0처리해버리는 렐룰\n",
    "print l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=tf.nn.softmax(l2) # 결과값을 확률화 한다.  #y의 형태는 784 X 10 즉 같다. \n",
    "\n",
    "#크로스 엔트로피를 평균낸다, reduce_mean = 평균, reduce_sum = 합, \n",
    "cross_entropy =tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "#크로스엔트로피를 줄이기 위해서 웨이치를 변화시킨다\n",
    "train_step =tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.arg_max(y,1) , tf.arg_max(y_ , 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction , tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zs , acc= [],[]\n",
    "init=tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc,zs,acc_val , zs_val,w1_mean_list =[],[],[],[],[]\n",
    "start_time=time.time()\n",
    "for i in range(100000):\n",
    "\n",
    "    batch_xs , batch_ys =next_batch( 60 , train_img , train_lab) \n",
    "    sess.run( train_step , feed_dict ={ x_: batch_xs , y_ :batch_ys })\n",
    "    if i%100 is 0:\n",
    "        train_res = sess.run([accuracy , z1 ] , feed_dict = {x_:batch_xs, y_ : batch_ys})\n",
    "        val_res   = sess.run([accuracy , z1 ] , feed_dict = {x_:val_img, y_ : val_lab})\n",
    "        mean   = sess.run(mean_w1 , feed_dict = {x_:val_img, y_ : val_lab})\n",
    "        \n",
    "        print 'step:',i, 'training',train_res[0],'           validation' , val_res[0] \n",
    "        acc.append(train_res[0])\n",
    "        zs.append(np.mean(train_res[1] , axis=0))\n",
    "        w1_mean_list.append(mean)\n",
    "        acc_val.append(val_res[0])\n",
    "        zs_val.append(np.mean(val_res[1] , axis=0))            \n",
    "end_time=time.time()\n",
    "zs , val_zs , acc, acc_val = np.array(zs) , np.array(zs_val) , np.array(acc) , np.array(acc_val)\n",
    "#when we test , through using above 4 parameter we get more higher accuracy \n",
    "test_res = sess.run([accuracy , z1 ] , feed_dict = {x_:test_img, y_ : test_lab})\n",
    "print 'test accuracy : ' , test_res[0] \n",
    "print 'final output ' , test_res[1]\n",
    "print 'The time taken by Training: ',end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print len(w1_mean_list)\n",
    "plt.plot(range(0,100000,100) , w1_mean_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
